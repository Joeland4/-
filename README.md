# 常见知识点
【个人理解】神经网路每一层都是进化着的分布  
【L1&L2正则化】：为什么减少过拟合？加入正则化项，在最小化经验误差的情况下，可以让我们选择解更简单（趋向于0）的解。详见：https://zhuanlan.zhihu.com/p/35356992   
【Scaler】：MinMax还是Standard？Standard能够避免异常值带来影响。详见：https://www.zhihu.com/question/20467170/answer/839255695  
【Norm】：BatchNorm,layerNorm,InstanceNorm,groupNorm 深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal  
 Covariate Shift。详见：https://zhuanlan.zhihu.com/p/87117010 | https://zhuanlan.zhihu.com/p/152232203  
【Activation function】：增加模型非线性详见：https://zhuanlan.zhihu.com/p/172254089  
【Learning rate】：  
【Gradient】：  
【Position embedding】：  

